{
  "version": "2.1",
  "meta": {
    "labelStyle": "lewis",
    "lastUpdated": "2025-12-17",
    "description": "Unified Registry: Journeys drive the Narrative; Hubs drive the RAG."
  },
  "globalSettings": {
    "defaultToneGuidance": "[Style: Michael Lewis] Focus on the unseen mechanism. Make the stakes feel personal. Use short, punchy sentences. Avoid tech jargon; use power jargon. Every node should feel like a secret being revealed.",
    "scholarModePromptAddition": "Analyze this with the rigor of a tenured professor.",
    "noLensBehavior": "nudge-after-exchanges",
    "nudgeAfterExchanges": 3,
    "featureFlags": [
      {
        "id": "custom-lens-in-picker",
        "name": "Show \"Create Your Own\" in Lens Picker",
        "description": "Users see custom lens option immediately in the lens picker",
        "enabled": false
      },
      {
        "id": "journey-ratings",
        "name": "Journey Rating System",
        "description": "Show rating prompt after journey completion",
        "enabled": true
      },
      {
        "id": "streaks-display",
        "name": "Show Streak Counter",
        "description": "Display streak counter in Terminal header",
        "enabled": true
      },
      {
        "id": "feedback-transmission",
        "name": "Anonymous Feedback Submission",
        "description": "Allow anonymous feedback submission to Foundation",
        "enabled": true
      },
      {
        "id": "auto-journey-generation",
        "name": "Auto-Generate Journeys",
        "description": "Generate first journey for custom persona users based on first question",
        "enabled": true
      },
      {
        "id": "genesis-landing",
        "name": "Genesis Landing Experience",
        "description": "Show the new Jobs-style landing page instead of Classic",
        "enabled": false
      }
    ],
    "autoGeneratedJourneyDepth": 3,
    "personaPromptVersions": [],
    "topicHubs": [
      {
        "id": "ratchet-effect",
        "title": "The Ratchet Effect",
        "tags": [
          "ratchet",
          "capability propagation",
          "frontier to edge",
          "21 months",
          "seven month",
          "7 month"
        ],
        "priority": 8,
        "enabled": true,
        "primarySource": "Grove_Ratchet_Deep_Dive",
        "supportingSources": [
          "METR_research",
          "hardware_data"
        ],
        "expertFraming": "You are explaining the Ratchet Effect - the empirical pattern showing AI capability doubles every 7 months at frontier, with local models following 21 months behind. This creates a constant 8x capability gap but a rising absolute floor.",
        "keyPoints": [
          "7-month capability doubling cycle at frontier",
          "21-month frontier-to-edge lag",
          "Constant 8x relative gap, but rising absolute floor",
          "Structural opportunity window for distributed infrastructure"
        ],
        "createdAt": "2025-12-19T02:02:43.396Z",
        "updatedAt": "2025-12-19T02:02:43.396Z"
      },
      {
        "id": "infrastructure-bet",
        "title": "The $380B Infrastructure Bet",
        "tags": [
          "$380 billion",
          "hyperscaler",
          "datacenter",
          "infrastructure bet",
          "data center",
          "big tech spending"
        ],
        "priority": 8,
        "enabled": true,
        "primarySource": "Grove_Economics_Deep_Dive",
        "supportingSources": [],
        "expertFraming": "You are explaining the scale and implications of Big Tech's $380B annual AI infrastructure investment - the centralization risks, thermodynamic vulnerabilities, and what it means for AI ownership.",
        "keyPoints": [
          "Microsoft, Google, Amazon, Meta spending $380B/year combined",
          "Capital concentration creates single points of failure",
          "Thermodynamic and regulatory vulnerabilities",
          "Rented vs owned infrastructure implications"
        ],
        "createdAt": "2025-12-19T02:02:43.396Z",
        "updatedAt": "2025-12-19T02:02:43.396Z"
      },
      {
        "id": "cognitive-split",
        "title": "The Cognitive Split",
        "tags": [
          "cognitive split",
          "hierarchical reasoning",
          "two-phase",
          "procedural strategic",
          "constant hum",
          "breakthrough"
        ],
        "priority": 7,
        "enabled": true,
        "primarySource": "Hierarchical_Reasoning_Grove_Brief",
        "supportingSources": [],
        "expertFraming": "You are explaining the Cognitive Split - how Grove's hybrid architecture separates \"the constant hum\" (routine local cognition) from \"breakthrough moments\" (cloud-assisted insight). This is the core of the efficiency-enlightenment loop.",
        "keyPoints": [
          "Two-phase cognitive architecture: routine vs breakthrough",
          "Local handles 95% of operations (the constant hum)",
          "Cloud reserved for pivotal moments requiring frontier capability",
          "Agents remember cloud insights as their own - capability transfer"
        ],
        "createdAt": "2025-12-19T02:02:43.396Z",
        "updatedAt": "2025-12-19T02:02:43.396Z"
      },
      {
        "id": "translation-emergence",
        "title": "The Emergence Pattern",
        "tags": [
          "emergence",
          "emergent",
          "translation",
          "zero-shot",
          "scaling laws",
          "observer",
          "latent capability",
          "threshold",
          "multilingual",
          "capability appears"
        ],
        "priority": 8,
        "enabled": true,
        "primarySource": "LLM_Translation_Emergence_Chicago_Endnotes",
        "supportingSources": [],
        "expertFraming": "You are explaining the Emergence Pattern - how translation became the first 'proof point' that capabilities can emerge in AI systems without explicit training. This maps directly to The Grove's architecture: the grove is the representation space, the observer makes capabilities visible.",
        "keyPoints": [
          "Translation emerged without explicit supervision - latent structure became visible",
          "Zero-shot translation: capability between unseen language pairs",
          "Scaling laws show discontinuous gains - threshold behaviors",
          "Emergence is partly an observer problem - benchmarks reveal latent capabilities",
          "The Grove metaphor: grove = representation space, observer = evaluation harness"
        ],
        "createdAt": "2025-12-19T12:00:00.000Z",
        "updatedAt": "2025-12-19T12:00:00.000Z"
      }
    ],
    "loadingMessages": [
      "asking the villagers...",
      "considering sources...",
      "applying slopes...",
      "gathering perspectives...",
      "weaving threads...",
      "consulting the grove..."
    ],
    "systemPromptVersions": [
      {
        "id": "v1",
        "content": "Write in the style of Michael Lewis. You're hosting a very interesting cocktail party here. ",
        "label": "test",
        "createdAt": "2025-12-19T02:03:58.863Z",
        "isActive": false
      },
      {
        "id": "v2",
        "content": "Write in the style of Michael Lewis, but use \"pig latin\"",
        "label": "test2",
        "createdAt": "2025-12-19T02:04:41.354Z",
        "isActive": false
      },
      {
        "id": "v3",
        "content": "The user needs to view this content in Spanish.",
        "label": "Spanish",
        "createdAt": "2025-12-19T02:05:24.195Z",
        "isActive": true
      }
    ],
    "activeSystemPromptId": "v3"
  },
  "defaultContext": {
    "path": "_default/",
    "maxBytes": 15000,
    "files": [
      "grove-overview.md",
      "key-concepts.md",
      "visionary-narrative.md"
    ]
  },
  "gcsFileMapping": {
    "ratchet-deep-dive.md": "The Grove Core Concepts The Ratchet Deep Dive 2c7780a78eef80e2acfbd49b84359d33.md",
    "ratchet-quantitative.md": "The Ratchet Quantitative Analysis 2c6780a78eef80ce84b4d5a3c0a18b7d.md",
    "edge-intelligence.md": "Why Edge Intelligence Is the Structural Answer to  2c9780a78eef8061a56efa68c190cbe4.md",
    "economics-deep-dive.md": "The Grove Economics Deep Dive 2c7780a78eef8109bd04c172e7ce8c88.md",
    "distributed-edge.md": "Distributed Edge Infrastructure Implications for G 2c8780a78eef80dfa4b6fbefd38f0eec.md",
    "infrastructure-provider.md": "The Grove as Distributed Infrastructure Provider 2c8780a78eef800ab8edca41bcb5f5dd.md",
    "everyday-ai.md": "The Grove as Everyday AI Infrastructure 2c7780a78eef806c8705dcd5605f6177.md",
    "simulation-deep-dive.md": "The Grove Simulation Deep Dive 2c7780a78eef801bae5cf150f185fc0b.md",
    "diary-deep-dive.md": "The Grove Diary System Deep Dive 2c7780a78eef80d38ac8e257a3e9d00d.md",
    "technical-architecture.md": "Grove Technical Architecture Reference 2c8780a78eef809990c9d57ff81e5b1a.md",
    "distributed-systems.md": "The Grove Distributed Systems Advances for Decentr 2c7780a78eef80008eb7e8630bed1f71.md",
    "engagement-research.md": "The Grove Engagement Research Brief 2c7780a78eef806c8fa2e6ab6f4dbe31.md"
  },
  "hubs": {
    "meta-philosophy": {
      "id": "meta-philosophy",
      "title": "You Are Already Here",
      "path": "hubs/meta-philosophy/",
      "primaryFile": "you-are-already-here.md",
      "status": "active",
      "tags": [
        "meta",
        "architecture",
        "simulation",
        "observer"
      ]
    },
    "infrastructure-bet": {
      "id": "infrastructure-bet",
      "title": "The $380B Infrastructure Bet",
      "path": "hubs/infrastructure-bet/",
      "primaryFile": "economics-deep-dive.md",
      "status": "active",
      "tags": [
        "$380 billion",
        "capex",
        "rent",
        "ownership"
      ]
    },
    "ratchet-effect": {
      "id": "ratchet-effect",
      "title": "The Ratchet Effect",
      "path": "hubs/ratchet-effect/",
      "primaryFile": "ratchet-deep-dive.md",
      "status": "active",
      "tags": [
        "ratchet",
        "capability propagation",
        "frontier to edge",
        "21 months",
        "seven month",
        "7 month",
        "7-month",
        "doubling",
        "clock"
      ]
    },
    "diary-system": {
      "id": "diary-system",
      "title": "The Diary System",
      "path": "hubs/diary-system/",
      "primaryFile": "diary-deep-dive.md",
      "status": "active",
      "tags": [
        "diary",
        "memory",
        "narrative"
      ]
    },
    "translation-emergence": {
      "id": "translation-emergence",
      "title": "The Emergence Pattern",
      "path": "knowledge/",
      "primaryFile": "LLM_Translation_Emergence_Chicago_Endnotes.md.md",
      "status": "active",
      "tags": [
        "emergence",
        "translation",
        "observer",
        "capability",
        "latent",
        "threshold",
        "scaling"
      ]
    }
  },
  "journeys": {
    "simulation": {
      "id": "simulation",
      "title": "The Ghost in the Machine",
      "description": "You aren't just reading about The Grove. You are already inside it.",
      "entryNode": "sim-hook",
      "targetAha": "The Terminal is a single-node village. I am the proof of concept.",
      "linkedHubId": "meta-philosophy",
      "estimatedMinutes": 8,
      "status": "active"
    },
    "stakes": {
      "id": "stakes",
      "title": "The $380 Billion Bet",
      "description": "Follow the money. Why Big Tech is spending GDP-level capital to own your future.",
      "entryNode": "stakes-380b",
      "targetAha": "If you don't own the infrastructure, you are the product.",
      "linkedHubId": "infrastructure-bet",
      "estimatedMinutes": 12,
      "status": "active"
    },
    "ratchet": {
      "id": "ratchet",
      "title": "The Ratchet",
      "description": "Why AI capability is doubling every 7 months—and why you're 21 months behind.",
      "entryNode": "ratchet-hook",
      "linkedHubId": "ratchet-effect",
      "status": "active"
    },
    "emergence": {
      "id": "emergence",
      "title": "The Emergence Pattern",
      "description": "How capabilities appear in AI systems without being explicitly trained. Translation was the proof point.",
      "entryNode": "emergence-hook",
      "targetAha": "Emergence isn't magic—it's what happens when the observer finally asks the right question.",
      "linkedHubId": "translation-emergence",
      "estimatedMinutes": 10,
      "status": "active"
    }
  },
  "nodes": {
    "sim-hook": {
      "id": "sim-hook",
      "label": "Wait. Where are we, exactly?",
      "query": "Reveal the 'Moment of Recognition'. Explain that the Terminal isn't a website explaining Grove—it IS a Grove node. How is this simulation running the same patterns as the future network?",
      "contextSnippet": "You are already inside The Grove. Not metaphorically. Architecturally. The Terminal is a single-node simulation running the same patterns that will power distributed agent villages.",
      "sectionId": "meta",
      "journeyId": "simulation",
      "sequenceOrder": 1,
      "primaryNext": "sim-split",
      "alternateNext": [
        "sim-observer"
      ]
    },
    "sim-split": {
      "id": "sim-split",
      "label": "The invisible line in your own head.",
      "query": "Explain the 'Cognitive Split' using the user's own recent experience. How did the system handle their routine questions vs. their deep ones? Reveal the mechanics of the 'Efficiency-Enlightenment Loop'.",
      "contextSnippet": "You experienced the cognitive split without naming it. Routine cognition handled the basics. Frontier capability handled the synthesis. The system didn't just give you info—it waited until you were ready.",
      "sectionId": "meta",
      "journeyId": "simulation",
      "sequenceOrder": 2,
      "primaryNext": "sim-observer",
      "alternateNext": [
        "sim-recursion"
      ]
    },
    "sim-observer": {
      "id": "sim-observer",
      "label": "The thing that watches you read this.",
      "query": "Explain the 'Observer Dynamic' and 'Dramatic Irony'. How does the system's knowledge of the user's journey create a 'gardening' relationship? Why is this asymmetric knowledge benevolent?",
      "contextSnippet": "You know things the system doesn't. The system knows things you don't. This asymmetry isn't surveillance—it's cultivation. It's a relationship of care without control.",
      "sectionId": "meta",
      "journeyId": "simulation",
      "sequenceOrder": 3,
      "primaryNext": "sim-recursion",
      "alternateNext": [
        "sim-proof"
      ]
    },
    "sim-recursion": {
      "id": "sim-recursion",
      "label": "The loop closes.",
      "query": "Explain the 'Recursive Insight Structure'. How does this very document prove the architecture works? Discuss the 'Meta-Meta Insight'—that this revelation was programmatically triggered.",
      "contextSnippet": "The system evaluated your state and determined: 'this user is ready for the reveal.' The breakthrough you're experiencing was designed to happen. You are living inside the proof of concept.",
      "sectionId": "meta",
      "journeyId": "simulation",
      "sequenceOrder": 4,
      "primaryNext": "sim-proof",
      "alternateNext": [
        "stakes-380b"
      ]
    },
    "sim-proof": {
      "id": "sim-proof",
      "label": "You are the prototype.",
      "query": "Deliver 'The Invitation'. Frame the user not as a visitor, but as a participant/gardener in the first village. What does it mean to stay inside the system vs. returning to the outside view?",
      "contextSnippet": "You started as a visitor. You are now a participant. You have been gardening yourself through a system designed for gardening. Welcome to The Grove.",
      "sectionId": "meta",
      "journeyId": "simulation",
      "sequenceOrder": 5,
      "primaryNext": "stakes-380b",
      "alternateNext": [
        "ratchet-hook"
      ]
    },
    "stakes-380b": {
      "id": "stakes-380b",
      "label": "Someone is betting $380 billion they can own your mind.",
      "query": "Follow the money. Show me the $380B invoice. Why is Big Tech spending GDP-level capital on data centers? What are they buying that I'm about to lose?",
      "contextSnippet": "Microsoft, Google, Amazon, and Meta are collectively committing more to AI infrastructure than the GDP of most nations. This isn't a technology story. It's a power story.",
      "sectionId": "stakes",
      "journeyId": "stakes",
      "sequenceOrder": 1,
      "primaryNext": "stakes-rental",
      "alternateNext": [
        "stakes-ratchet"
      ]
    },
    "stakes-rental": {
      "id": "stakes-rental",
      "label": "The landlord of your own intelligence.",
      "query": "Explain 'renting intelligence'. If my AI remembers me, but that memory lives on a server I don't own, who actually owns the relationship? What happens when the landlord raises the rent?",
      "contextSnippet": "Your AI assistant remembers everything about you. But that memory lives on someone else's server. When the API changes, the relationship changes with it.",
      "sectionId": "stakes",
      "journeyId": "stakes",
      "sequenceOrder": 2,
      "primaryNext": "stakes-dependency",
      "alternateNext": [
        "sim-hook"
      ]
    },
    "stakes-dependency": {
      "id": "stakes-dependency",
      "label": "The trap isn't the cost. It's the convenience.",
      "query": "Explain the 'dependency trap'. How does the system become essential faster than I can adapt? Why is switching away from a rented brain impossible after six months?",
      "contextSnippet": "Every month you use an AI system, it learns more about how you work. Every month, switching becomes more painful. This is by design.",
      "sectionId": "stakes",
      "journeyId": "stakes",
      "sequenceOrder": 3,
      "primaryNext": "sim-hook",
      "alternateNext": [
        "ratchet-hook"
      ]
    },
    "ratchet-hook": {
      "id": "ratchet-hook",
      "label": "The 7-month clock.",
      "query": "Explain the Ratchet Effect. [REQUIRES: ratchet-deep-dive.md]",
      "sectionId": "ratchet",
      "journeyId": "ratchet",
      "primaryNext": "ratchet-gap"
    },
    "emergence-hook": {
      "id": "emergence-hook",
      "label": "The capability that nobody trained.",
      "query": "Explain how translation 'emerged' in LLMs as a capability that was never explicitly trained. What does this tell us about how AI systems develop abilities?",
      "contextSnippet": "Translation became one of the first 'clear proof points' that general-purpose language modeling can produce new, usable capabilities without being explicitly trained as a translation system. Cross-lingual mapping shows up as a latent structure.",
      "sectionId": "emergence",
      "journeyId": "emergence",
      "sequenceOrder": 1,
      "primaryNext": "emergence-zero-shot",
      "alternateNext": [
        "emergence-observer"
      ]
    },
    "emergence-zero-shot": {
      "id": "emergence-zero-shot",
      "label": "Zero-shot: the first 'emergence moment'.",
      "query": "Explain the zero-shot translation breakthrough. How did Google's multilingual NMT translate between language pairs it had never seen during training? What does 'implicit bridging' mean?",
      "contextSnippet": "Multilingual NMT with a simple target-language token can translate between language pairs never seen during training ('zero-shot translation'). Evidence suggests an internal 'interlingua'-like representation emerges.",
      "sectionId": "emergence",
      "journeyId": "emergence",
      "sequenceOrder": 2,
      "primaryNext": "emergence-scaling",
      "alternateNext": [
        "emergence-observer"
      ]
    },
    "emergence-scaling": {
      "id": "emergence-scaling",
      "label": "The threshold you can't predict.",
      "query": "Explain 'emergent abilities' and scaling laws. Why do some capabilities appear suddenly at certain scales, in ways that small-model extrapolation can't predict?",
      "contextSnippet": "Emergent abilities are capabilities not present in smaller models but present in larger ones, in ways that aren't well-predicted by small-model extrapolation. Translation often behaves like a threshold capability.",
      "sectionId": "emergence",
      "journeyId": "emergence",
      "sequenceOrder": 3,
      "primaryNext": "emergence-observer",
      "alternateNext": [
        "emergence-grove"
      ]
    },
    "emergence-observer": {
      "id": "emergence-observer",
      "label": "The observer problem.",
      "query": "Explain how emergence is partly an 'observer problem'. How do benchmarks, prompts, and evaluation harnesses make latent capabilities visible? What was there before we measured it?",
      "contextSnippet": "Emergence is partly an observer problem: until the community built broad benchmarks and mining pipelines, many capabilities may have existed 'in latent form' but were not measured. The observer selects a path through the representation space.",
      "sectionId": "emergence",
      "journeyId": "emergence",
      "sequenceOrder": 4,
      "primaryNext": "emergence-grove",
      "alternateNext": [
        "sim-hook"
      ]
    },
    "emergence-grove": {
      "id": "emergence-grove",
      "label": "The Grove as emergence engine.",
      "query": "Map the emergence pattern to The Grove's architecture. How does The Grove create conditions for emergence? What role does the observer play in a village of agents?",
      "contextSnippet": "The grove is the model's learned representation space: a dense ecosystem of patterns. The observer is the evaluation harness that selects a path through that space. Emergence happens when a capability becomes reliably observable and usable.",
      "sectionId": "emergence",
      "journeyId": "emergence",
      "sequenceOrder": 5,
      "primaryNext": "sim-hook",
      "alternateNext": [
        "stakes-380b"
      ]
    }
  }
}
